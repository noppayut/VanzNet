{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'vanznames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath, 'r') as f:\n",
    "    names_raw = list(map(lambda x: x.replace('\\n', ''), f.readlines()))\n",
    "    \n",
    "all_letters = list(set(\"\".join(names_raw)))\n",
    "n_letters = len(all_letters) + 1 # including EOS\n",
    "\n",
    "def encode_name(name):\n",
    "    return [all_letters.index(s) for s in name]\n",
    "\n",
    "names_enc = [encode_name(name) for name in names_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "[51, 129, 28, 67, 30, 44, 19, 4, 96, 1, 4, 57, 4, 41, 57, 51, 4, 44, 141, 11]\n"
     ]
    }
   ],
   "source": [
    "print(n_letters)\n",
    "print(names_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "class VanzNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dor=0.15, bid=True):\n",
    "        super(VanzNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=3\n",
    "                            , dropout=dor, bidirectional=bid)\n",
    "        if bid:\n",
    "            input_mult = 2\n",
    "        else:\n",
    "            input_mult = 1\n",
    "        self.lstm2o = nn.Linear(hidden_size*input_mult, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, input, h, c):\n",
    "        if h is None:            \n",
    "            output_lstm, (h_out, c_out) = self.lstm(input)\n",
    "        else:\n",
    "            output_lstm, (h_out, c_out)  = self.lstm(input, (h, c))\n",
    "        output_fc = self.lstm2o(output_lstm)\n",
    "        output = self.softmax(output_fc)\n",
    "        return output, h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot matrix for characters\n",
    "def name2input(name_enc):\n",
    "    \"\"\"    \n",
    "    dim: [len_name * 1 * n_letters]\n",
    "    e.g. KASPAROV -> [OneHot(K), OneHot(A), ..., OneHot(V)]\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(name_enc), 1, n_letters)\n",
    "    for i, n in enumerate(name_enc):\n",
    "        tensor[i][0][n] = 1\n",
    "    return tensor\n",
    "\n",
    "def name2target(name_enc):\n",
    "    \"\"\"\n",
    "    dim: [len_name]\n",
    "    e.g. target(KASPAROV) -> ASPAROV<EOS> -> [Idx(A), Idx(S), ..., Idx(EOS)]\n",
    "    \"\"\"\n",
    "    return torch.LongTensor(name_enc[1:] + [n_letters - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "rnn = VanzNet(n_letters, hidden_size, n_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/200: Loss 1009775.000000\n",
      "4/200: Loss 861443.687500\n",
      "6/200: Loss 757746.687500\n",
      "8/200: Loss 663345.562500\n",
      "10/200: Loss 583207.250000\n",
      "12/200: Loss 507072.718750\n",
      "14/200: Loss 451895.968750\n",
      "16/200: Loss 402872.937500\n",
      "18/200: Loss 356772.750000\n",
      "20/200: Loss 326195.437500\n",
      "22/200: Loss 298238.875000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d10cac9582a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 200\n",
    "print_every = 2\n",
    "max_training_size = -1\n",
    "lr_decay_step = 5\n",
    "lr_decay_rate = 0.88\n",
    "\n",
    "if max_training_size > 0:\n",
    "    names_train = names_enc[:max_training_size]\n",
    "else:\n",
    "    names_train = names_enc\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim\n",
    "                                            , lr_decay_step\n",
    "                                            , lr_decay_rate)\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0    \n",
    "    for name in names_train:\n",
    "        input_tensor = name2input(name)        \n",
    "        target_tensor = name2target(name)\n",
    "        target_tensor.unsqueeze_(-1)\n",
    "        optim.zero_grad()\n",
    "        loss = 0\n",
    "        h, c = None, None\n",
    "        for i in range(input_tensor.size(0)):\n",
    "            output, h, c = rnn(input_tensor[i].view(1,1, -1), h, c)\n",
    "            # print(output.size(), target_tensor[i].size())\n",
    "            loss += criterion(output.view(1, -1), target_tensor[i])\n",
    "            loss_epoch += loss\n",
    "        loss_epoch /= input_tensor.size(0)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    losses.append(loss_epoch / len(names_train))\n",
    "    \n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(\"%d/%d: Loss %f\" % (epoch+1, epochs, loss_epoch))\n",
    "        # print(output.topk(1)[1])\n",
    "        # print(target_tensor)\n",
    "        # print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_char(output):\n",
    "    topv, topi = output.topk(1)\n",
    "    idx = topi[0][0][0].item()\n",
    "    # print(idx)\n",
    "    if idx == n_letters - 1:\n",
    "        # return all_letters[topi[0][0][1].item()]\n",
    "        return 'EOS'\n",
    "    else:\n",
    "        return all_letters[idx]\n",
    "\n",
    "def sample_name(start_char):\n",
    "    name_sample = start_char\n",
    "    if not (start_char in all_letters):\n",
    "        return \"Invalid start character!\"\n",
    "    else:\n",
    "        start_char_enc = [all_letters.index(start_char)]\n",
    "        input_tensor = name2input(start_char_enc)\n",
    "        out_char = \"\"\n",
    "        h, c = None, None\n",
    "        \n",
    "        while True:\n",
    "            output, h, c = rnn(input_tensor[0].view(1,1,-1), h, c)\n",
    "            out_char = reconstruct_char(output)\n",
    "            if out_char == 'EOS':\n",
    "                break\n",
    "            name_sample += out_char\n",
    "            input_tensor = name2input([all_letters.index(out_char)])\n",
    "    \n",
    "    return name_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กิตติศักดิ์ กันทะวัง\n",
      "คน บ้า\n",
      "มินิ กองฟาง\n",
      "จั้ก บุญจอง\n",
      "วัน ทูกแป้ว\n",
      "รักกันเมื่อยังหาย ใจ\n",
      "บอย ก็กี\n"
     ]
    }
   ],
   "source": [
    "start_letters = ['ก', 'ค', 'ม', 'จ', 'ว', 'ร', 'บ']\n",
    "# start_letters = all_letters\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sp in start_letters:\n",
    "        print(sample_name(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
