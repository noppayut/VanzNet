{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'vanznames.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "import unicodedata\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    names_raw = list(map(lambda x: x.replace('\\n', ''), f.readlines()))\n",
    "    \n",
    "all_letters = list(set(\"\".join(names_raw)))\n",
    "n_letters = len(all_letters) + 1 # including EOS\n",
    "\n",
    "def encode_name(name):\n",
    "    return [all_letters.index(s) for s in name]\n",
    "\n",
    "names_enc = [encode_name(name) for name in names_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "[35, 126, 97, 112, 124, 67, 105, 140, 15, 132, 140, 100, 140, 148, 100, 35, 140, 67, 114, 121]\n"
     ]
    }
   ],
   "source": [
    "print(n_letters)\n",
    "print(names_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "class VanzNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dor=0.15):\n",
    "        super(VanzNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        total_input_size = input_size + hidden_size\n",
    "        self.i2h = nn.Linear(total_input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(total_input_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dor)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot matrix for characters\n",
    "def name2input(name_enc):\n",
    "    \"\"\"    \n",
    "    dim: [len_name * 1 * n_letters]\n",
    "    e.g. KASPAROV -> [OneHot(K), OneHot(A), ..., OneHot(V)]\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(name_enc), 1, n_letters)\n",
    "    for i, n in enumerate(name_enc):\n",
    "        tensor[i][0][n] = 1\n",
    "    return tensor\n",
    "\n",
    "def name2target(name_enc):\n",
    "    \"\"\"\n",
    "    dim: [len_name]\n",
    "    e.g. target(KASPAROV) -> ASPAROV<EOS> -> [Idx(A), Idx(S), ..., Idx(EOS)]\n",
    "    \"\"\"\n",
    "    return torch.LongTensor(name_enc[1:] + [n_letters - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/5000: Loss 25843.703125\n",
      "100/5000: Loss 16845.751953\n",
      "150/5000: Loss 12976.521484\n",
      "200/5000: Loss 12745.041992\n",
      "250/5000: Loss 10902.885742\n",
      "300/5000: Loss 11982.556641\n",
      "350/5000: Loss 10998.518555\n",
      "400/5000: Loss 10979.769531\n",
      "450/5000: Loss 13333.736328\n",
      "500/5000: Loss 12522.534180\n",
      "550/5000: Loss 10985.856445\n",
      "600/5000: Loss 12639.764648\n",
      "650/5000: Loss 11052.083984\n",
      "700/5000: Loss 13228.370117\n",
      "750/5000: Loss 11985.356445\n",
      "800/5000: Loss 12747.591797\n",
      "850/5000: Loss 11505.390625\n",
      "900/5000: Loss 12674.726562\n",
      "950/5000: Loss 11368.822266\n",
      "1000/5000: Loss 11612.627930\n",
      "1050/5000: Loss 16415.802734\n",
      "1100/5000: Loss 11769.797852\n",
      "1150/5000: Loss 11546.096680\n",
      "1200/5000: Loss 11653.357422\n",
      "1250/5000: Loss 13433.497070\n",
      "1300/5000: Loss 17556.208984\n",
      "1350/5000: Loss 11598.698242\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "epochs = 5000\n",
    "print_every = 50\n",
    "max_training_size = 100\n",
    "\n",
    "if max_training_size > 0:\n",
    "    names_train = names_enc[:max_training_size]\n",
    "else:\n",
    "    names_train = names_enc\n",
    "\n",
    "rnn = VanzNet(n_letters, 128, n_letters)\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "losses = []\n",
    "for epoch in range(epochs):    \n",
    "    loss_epoch = 0\n",
    "    for name in names_train:\n",
    "        input_tensor = name2input(name)\n",
    "        target_tensor = name2target(name)\n",
    "        target_tensor.unsqueeze_(-1)\n",
    "        # print(input_tensor)\n",
    "        # print(target_tensor)\n",
    "        hidden = rnn.initHidden()        \n",
    "        optim.zero_grad()\n",
    "        loss = 0\n",
    "        for i in range(input_tensor.size(0)):\n",
    "            output, hidden = rnn(input_tensor[i], hidden)\n",
    "            loss += criterion(output, target_tensor[i])\n",
    "            loss_epoch += loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    losses.append(loss_epoch / len(names_train))\n",
    "    \n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(\"%d/%d: Loss %f\" % (epoch+1, epochs, loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_char(output):\n",
    "    topv, topi = output.topk(1)\n",
    "    idx = topi[0][0]\n",
    "    if idx == n_letters - 1:\n",
    "        return 'EOS'\n",
    "    else:\n",
    "        return all_letters[idx]\n",
    "\n",
    "def sample_name(start_char):\n",
    "    name_sample = start_char\n",
    "    if not (start_char in all_letters):\n",
    "        return \"Invalid start character!\"\n",
    "    else:\n",
    "        start_char_enc = [all_letters.index(start_char)]\n",
    "        input_tensor = name2input(start_char_enc)\n",
    "        hidden = rnn.initHidden()\n",
    "        out_char = \"\"\n",
    "        \n",
    "        while out_char != 'EOS':            \n",
    "            output, hidden = rnn(input_tensor, hidden)\n",
    "            out_char = reconstruct_char(output)\n",
    "            name_sample += out_char\n",
    "            input_tensor = name2input(out_char)\n",
    "    \n",
    "    return name_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = ['ก', 'ค', 'ม']\n",
    "\n",
    "for sp in samplers:\n",
    "    print(sample_name(sp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
